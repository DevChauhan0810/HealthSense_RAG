{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzQeGMxgN96y"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers faiss-cpu biopython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu --no-cache-dir\n",
        "!pip install sentence-transformers biopython"
      ],
      "metadata": {
        "id": "h3UYUOZPTux9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade protobuf==3.20.3"
      ],
      "metadata": {
        "id": "sHyZqdjRTxre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)\n",
        "# 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   TOKEN"
      ],
      "metadata": {
        "id": "_hywnaF-jREs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "HF_MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "MODEL_DIR = \"/meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Download tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    HF_MODEL_ID,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "# Download model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    HF_MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Save to Google Drive\n",
        "tokenizer.save_pretrained(MODEL_DIR)\n",
        "model.save_pretrained(MODEL_DIR)\n",
        "\n",
        "print(\"Model downloaded and stored at:\", MODEL_DIR)\n"
      ],
      "metadata": {
        "id": "yGhP_ZuRjR2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from Bio import Entrez\n",
        "\n",
        "Entrez.email = \"khushpatel1080@gmail.com\"\n",
        "\n",
        "\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qulGf9waTxtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self, model_name: str = EMBED_MODEL_NAME):\n",
        "        # SentenceTransformers normalizes embeddings if asked; we'll normalize here explicitly\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def encode(self, texts: List[str]) -> np.ndarray:\n",
        "        emb = self.model.encode(\n",
        "            texts,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,  # cosine similarity via inner product\n",
        "        )\n",
        "        return emb.astype(\"float32\")\n"
      ],
      "metadata": {
        "id": "GhjhTMlsTx0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bio"
      ],
      "metadata": {
        "id": "QoRc-MUMTx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PubMedClient:\n",
        "    def __init__(self, max_results: int = 5):\n",
        "        self.max_results = max_results\n",
        "\n",
        "    def fetch_docs(self, query: str) -> List[Dict[str, Any]]:\n",
        "        # 1) ESearch: get PubMed IDs\n",
        "        handle = Entrez.esearch(\n",
        "            db=\"pubmed\",\n",
        "            term=query,\n",
        "            retmax=self.max_results,\n",
        "            sort=\"relevance\",\n",
        "            retmode=\"xml\",\n",
        "        )\n",
        "        search_record = Entrez.read(handle)\n",
        "        ids = search_record.get(\"IdList\", [])\n",
        "        handle.close()\n",
        "\n",
        "        if not ids:\n",
        "            return []\n",
        "\n",
        "        handle = Entrez.efetch(\n",
        "            db=\"pubmed\",\n",
        "            id=\",\".join(ids),\n",
        "            rettype=\"abstract\",\n",
        "            retmode=\"text\",\n",
        "        )\n",
        "        raw_text = handle.read()\n",
        "        handle.close()\n",
        "\n",
        "        chunks = [c.strip() for c in raw_text.split(\"\\n\\n\") if c.strip()]\n",
        "        docs = []\n",
        "        for pmid, chunk in zip(ids, chunks):\n",
        "            docs.append(\n",
        "                {\n",
        "                    \"id\": pmid,\n",
        "                    \"title\": \"\",\n",
        "                    \"abstract\": chunk,\n",
        "                }\n",
        "            )\n",
        "        return docs\n"
      ],
      "metadata": {
        "id": "GDCqywQ2Tx48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from Bio import Entrez\n",
        "\n",
        "class PDFDownloader:\n",
        "    def __init__(self, download_dir=\"/kaggle/working/downloaded_pdfs\"):\n",
        "        self.download_dir = download_dir\n",
        "        os.makedirs(self.download_dir, exist_ok=True)\n",
        "\n",
        "    def get_pmc_id(self, pmid):\n",
        "        \"\"\"Converts a PubMed ID (PMID) to a PubMed Central ID (PMCID) for Open Access download.\"\"\"\n",
        "        try:\n",
        "            handle = Entrez.elink(dbfrom=\"pubmed\", db=\"pmc\", linkname=\"pubmed_pmc\", id=pmid)\n",
        "            result = Entrez.read(handle)\n",
        "            handle.close()\n",
        "            # Extract PMC ID if a link exists\n",
        "            if result and result[0]['LinkSetDb']:\n",
        "                return result[0]['LinkSetDb'][0]['Link'][0]['Id']\n",
        "        except Exception as e:\n",
        "            print(f\"Could not find PMC ID for PMID {pmid}: {e}\")\n",
        "        return None\n",
        "\n",
        "    def download_pdf(self, pmid, title):\n",
        "        \"\"\"Attempts to download the PDF if a PMC ID is found, skipping if file exists.\"\"\"\n",
        "\n",
        "        # Check if file already exists (skipping logic not fully shown in your paste, but good practice)\n",
        "        safe_title = \"\".join([c for c in title if c.isalnum() or c in (' ','-')]).rstrip()\n",
        "        pmc_id_placeholder = \"NO_PMC\" # Placeholder until we know the real ID\n",
        "        filename = f\"{self.download_dir}/{safe_title[:50]}_{pmc_id_placeholder}.pdf\"\n",
        "\n",
        "        # 1. ATTEMPT TO GET PMC ID\n",
        "        pmc_id = self.get_pmc_id(pmid)\n",
        "\n",
        "        if not pmc_id:\n",
        "            print(f\"PDF download skipped for '{title[:30]}...' (No PMC ID found for PMID {pmid}). The article is likely **not archived in PubMed Central (PMC)**.\")\n",
        "            return\n",
        "\n",
        "        # Update filename with actual PMCID\n",
        "        filename = f\"{self.download_dir}/{safe_title[:50]}_PMC{pmc_id}.pdf\"\n",
        "\n",
        "        # 2. CHECK FOR EXISTING FILE\n",
        "        if os.path.exists(filename):\n",
        "            print(f\"[SKIP-2] PDF already in storage: {filename}\")\n",
        "            return\n",
        "\n",
        "        # 3. ATTEMPT TO DOWNLOAD OPEN ACCESS PDF\n",
        "        pdf_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/pdf/\"\n",
        "\n",
        "        try:\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (ColabUser; mailto:khushpatel1080@gmail.com)'}\n",
        "            response = requests.get(pdf_url, headers=headers, stream=True)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"[SUCCESS] Downloaded: {filename}\")\n",
        "            else:\n",
        "                # Most common failure reason is a 403 Forbidden for paywalled content\n",
        "                print(f\"[FAIL-3] Could not retrieve Open Access PDF for PMC{pmc_id}. Status: {response.status_code}. **The article is likely paywalled.** URL: {pdf_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] downloading PMC{pmc_id}: {e}\")"
      ],
      "metadata": {
        "id": "Inj6Mmc0Tx7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "\n",
        "reward_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n"
      ],
      "metadata": {
        "id": "E7ObuKL1sUhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top3_docs(query: str, docs: list, reward_model) -> list:\n",
        "    \"\"\"\n",
        "    docs: List of dicts with 'text', 'title', 'external_id'\n",
        "    Returns top-3 docs based on reward model scores\n",
        "    \"\"\"\n",
        "    if len(docs) <= 3:\n",
        "        return docs  # nothing to select\n",
        "\n",
        "    pairs = [(query, doc['text']) for doc in docs]\n",
        "    scores = reward_model.predict(pairs)\n",
        "    top3_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
        "    top3_docs = [docs[i] for i in top3_idx]\n",
        "    return top3_docs\n"
      ],
      "metadata": {
        "id": "s08pDSHgsVqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsPsudgOsZ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DocumentStore:\n",
        "    def __init__(self, embedder: Embedder, dim: int):\n",
        "        self.embedder = embedder\n",
        "        self.dim = dim\n",
        "\n",
        "\n",
        "        self.keyword_index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
        "        self.doc_index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
        "\n",
        "\n",
        "        self.meta: Dict[int, Dict[str, Any]] = {}\n",
        "\n",
        "        self._next_local_id = 1\n",
        "\n",
        "    def _alloc_id(self) -> int:\n",
        "        doc_id = self._next_local_id\n",
        "        self._next_local_id += 1\n",
        "        return doc_id\n",
        "\n",
        "    def add_document(self, text: str, keyword_text: str, external_id: str = None) -> int:\n",
        "        \"\"\"\n",
        "        text         : full document text (e.g., title + abstract)\n",
        "        keyword_text : the query / keywords used to fetch this doc\n",
        "        external_id  : optional external ID (e.g., PubMed PMID)\n",
        "        \"\"\"\n",
        "        # 0th index = keyword embedding\n",
        "        keyword_emb = self.embedder.encode([keyword_text])[0]\n",
        "        # 1st index = full document embedding\n",
        "        doc_emb = self.embedder.encode([text])[0]\n",
        "\n",
        "        # Store as a small 2 x dim matrix: [0] keyword, [1] full doc\n",
        "        embeddings = np.stack([keyword_emb, doc_emb], axis=0)  # shape (2, dim)\n",
        "\n",
        "        doc_id = self._alloc_id()\n",
        "\n",
        "        self.meta[doc_id] = {\n",
        "            \"text\": text,\n",
        "            \"keyword_text\": keyword_text,\n",
        "            \"external_id\": external_id,\n",
        "            \"embeddings\": embeddings,   # here '0th index' is embeddings[0]\n",
        "        }\n",
        "\n",
        "        # Add to FAISS indexes (one vector per doc per index)\n",
        "        self.keyword_index.add_with_ids(\n",
        "            keyword_emb[None, :],\n",
        "            np.array([doc_id], dtype=\"int64\"),\n",
        "        )\n",
        "        self.doc_index.add_with_ids(\n",
        "            doc_emb[None, :],\n",
        "            np.array([doc_id], dtype=\"int64\"),\n",
        "        )\n",
        "\n",
        "        return doc_id\n",
        "\n",
        "    def search_local(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 5,\n",
        "        keyword_threshold: float = 0.4,\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Search only in local FAISS indexes.\n",
        "        Stage 1: compare query embedding with 0th-index (keyword) embeddings.\n",
        "        \"\"\"\n",
        "        q_emb = self.embedder.encode([query]).astype(\"float32\")  # shape (1, dim)\n",
        "\n",
        "        # Keyword-level FAISS search\n",
        "        D_kw, I_kw = self.keyword_index.search(q_emb, top_k)\n",
        "\n",
        "        sims = D_kw[0]\n",
        "        ids = I_kw[0]\n",
        "        mask = sims >= keyword_threshold\n",
        "\n",
        "        filtered_ids = ids[mask]\n",
        "        filtered_sims = sims[mask]\n",
        "\n",
        "        results = []\n",
        "        for doc_id, sim in zip(filtered_ids.tolist(), filtered_sims.tolist()):\n",
        "            if doc_id == -1:\n",
        "                continue\n",
        "            meta = self.meta.get(doc_id)\n",
        "            if not meta:\n",
        "                continue\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"similarity_keyword\": float(sim),\n",
        "                    \"text\": meta[\"text\"],\n",
        "                    \"keyword_text\": meta[\"keyword_text\"],\n",
        "                    \"external_id\": meta[\"external_id\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "        return results\n",
        "    def search_with_pubmed_backfill(\n",
        "        self,\n",
        "        query: str,\n",
        "        pubmed_client: PubMedClient,\n",
        "        top_k: int = 5,\n",
        "        keyword_threshold: float = 0.4,\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Try local FAISS search; if nothing passes the threshold,\n",
        "        call PubMed, store new docs, and then search again.\n",
        "        \"\"\"\n",
        "        print(f\"Checking local vector store for query: '{query}'...\")\n",
        "\n",
        "\n",
        "        local_results = self.search_local(\n",
        "            query, top_k=top_k, keyword_threshold=keyword_threshold\n",
        "        )\n",
        "\n",
        "\n",
        "        if local_results:\n",
        "            print(f\" -> Found {len(local_results)} documents in local storage (Similarity >= {keyword_threshold}).\")\n",
        "            print(\" -> Skipping external PubMed search.\")\n",
        "            return local_results\n",
        "\n",
        "\n",
        "        print(\" -> No sufficient local matches found. Fetching from external PubMed API...\")\n",
        "\n",
        "\n",
        "        pubmed_docs = pubmed_client.fetch_docs(query)\n",
        "        print(f\" -> PubMed API retrieved {len(pubmed_docs)} documents.\")\n",
        "\n",
        "        initial_index_size = self.keyword_index.ntotal\n",
        "        print(f\" FAISS Index Size BEFORE adding: {initial_index_size}\")\n",
        "\n",
        "        for i, d in enumerate(pubmed_docs):\n",
        "            full_text = f\"{d.get('title', '')}\\n{d.get('abstract', '')}\"\n",
        "            self.add_document(\n",
        "                text=full_text,\n",
        "                keyword_text=query,\n",
        "                external_id=d.get(\"id\"),\n",
        "            )\n",
        "\n",
        "            print(f\" Successfully added document {i+1}/{len(pubmed_docs)}: PMID {d.get('id')}\")\n",
        "\n",
        "        final_index_size = self.keyword_index.ntotal\n",
        "        print(f\"  FAISS Index Size AFTER adding: {final_index_size}\")\n",
        "\n",
        "\n",
        "        return self.search_local(query, top_k=top_k, keyword_threshold=keyword_threshold)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KDefRb0oTx9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rag_prompt(user_query: str, retrieved_docs: List[Dict[str, Any]]) -> str:\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return f\"User question: {user_query}\\nAnswer: I could not find any relevant documents to answer your question.\"\n",
        "\n",
        "\n",
        "    context_str = \"\"\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        context_str += f\"Source {i}: [Title: {doc.get('title', 'Unknown')}]\\nContent: {doc['text']}\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a research assistant. Answer the user question based ONLY on the provided Context sources.\n",
        "\n",
        "Instructions:\n",
        "1. You must base your answer strictly on the provided sources.\n",
        "2. You MUST cite the specific Source Number or Title when stating facts (e.g., \"According to Source 1...\").\n",
        "3. At the end of your answer, list the titles of the articles you used.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "User question:\n",
        "{user_query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return prompt.strip()"
      ],
      "metadata": {
        "id": "yMP8UKuiTyAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMClient:\n",
        "    def __init__(self, model_id: str = \"/kaggle/working/Llama-2-7b-hf\"):\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "        self.model_id = model_id\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "        self.pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    def generate(self, prompt: str, max_tokens: int = 256) -> str:\n",
        "        outputs = self.pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "        # text-generation pipeline returns a list of dicts with 'generated_text'\n",
        "        full_text = outputs[0][\"generated_text\"]\n",
        "        # Strip the original prompt, keep only the completion if needed\n",
        "        return full_text[len(prompt):].strip()\n"
      ],
      "metadata": {
        "id": "J3g642zvTyCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_query(\n",
        "    user_query: str,\n",
        "    doc_store: DocumentStore,\n",
        "    pubmed_client: PubMedClient,\n",
        "    llm_client: LLMClient,\n",
        "    top_k: int = 5,\n",
        "    keyword_threshold: float = 0.4,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    End-to-end:\n",
        "    - embed query\n",
        "    - search local FAISS (cache check)\n",
        "    - if no results, call PubMed API\n",
        "    - DOWNLOAD PDFS for the found articles (New Step)\n",
        "    - build RAG prompt\n",
        "    - generate answer\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    retrieved = doc_store.search_with_pubmed_backfill(\n",
        "      query=user_query,\n",
        "      pubmed_client=pubmed_client,\n",
        "      top_k=10,\n",
        "      keyword_threshold=keyword_threshold,\n",
        ")\n",
        "\n",
        "# Selecting  best 3 using reward model\n",
        "      top3_docs = select_top3_docs(user_query, retrieved, reward_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"\\n[System] Found {len(retrieved)} relevant abstracts.\")\n",
        "\n",
        "    # Initialize the downloader class we created earlier\n",
        "    downloader = PDFDownloader()\n",
        "\n",
        "    # Loop through every article found to get its full PDF\n",
        "    for doc in retrieved:\n",
        "        pmid = doc.get('external_id')    # Get the PubMed ID\n",
        "        title = doc.get('title', 'Untitled')\n",
        "\n",
        "        if pmid:\n",
        "            # This triggers the download (or skips if file exists)\n",
        "            downloader.download_pdf(pmid, title)\n",
        "\n",
        "    prompt = build_rag_prompt(user_query, top3_docs)\n",
        "    answer = llm_client.generate(prompt)\n",
        "\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "Yeb75PbjW_d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "embedder = Embedder(EMBED_MODEL_NAME)\n",
        "dim = embedder.encode([\"test\"]).shape[1]   # infer embedding dimension\n",
        "\n",
        "doc_store = DocumentStore(embedder=embedder, dim=dim)\n",
        "pubmed_client = PubMedClient(max_results=3)\n",
        "llm_client = LLMClient()\n",
        "\n",
        "print(\"Embedding dimension:\", dim)\n"
      ],
      "metadata": {
        "id": "lGOV1jfZTyEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss.write_index(doc_store.keyword_index, \"/kaggle/working/keyword.index\")\n",
        "faiss.write_index(doc_store.doc_index, \"/kaggle/working/doc.index\")\n",
        "\n",
        "import pickle, os\n",
        "with open(\"/kaggle/working/metadata.pkl\", \"wb\") as f:\n",
        "    pickle.dump(doc_store.meta, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k57nBNk5fOxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_store.keyword_index = faiss.read_index(\"/kaggle/working/keyword.index\")\n",
        "doc_store.doc_index = faiss.read_index(\"/kaggle/working/doc.index\")\n",
        "\n",
        "import pickle\n",
        "with open(\"/kaggle/working/metadata.pkl\", \"rb\") as f:\n",
        "    doc_store.meta = pickle.load(f)\n",
        "\n",
        "doc_store._next_local_id = max(doc_store.meta.keys(), default=0) + 1\n"
      ],
      "metadata": {
        "id": "VvbAvs30fSoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use only the search keywords\n",
        "search_term = \"diabetes caused by sugar\"\n",
        "\n",
        "print(\"--- RUNNING QUERY (Clean Search Term) ---\")\n",
        "answer = answer_query(\n",
        "    # Only send keywords to the search/retrieval system\n",
        "    user_query=search_term,\n",
        "    doc_store=doc_store,\n",
        "    pubmed_client=pubmed_client,\n",
        "    llm_client=llm_client,\n",
        "    top_k=3,\n",
        "    keyword_threshold=0.4,\n",
        ")\n",
        "\n",
        "print(\"\\nUser search term:\")\n",
        "print(search_term)\n",
        "print(\"\\nSystem answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "f14bDg9fZfNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYgN81_oZfSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yTpJQE-ZfUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WlGBVZGZfWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ut1zgUajZfZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,  # VRAM efficient\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# LoRA configuration (small adapter for formatting only)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"template_dataset.json\")[\"train\"]\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(example):\n",
        "    input_text = f\"Query: {example['query']}\\nContext: {example['context']}\\nAnswer:\"\n",
        "    target_text = example['output']\n",
        "    input_ids = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=256).input_ids\n",
        "    labels = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=128).input_ids\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "# Training arguments (small number of epochs for formatting only)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sft_template_llama7b\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,  # Only formatting, 1 epoch enough\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_steps=100,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./sft_template_llama7b\")\n"
      ],
      "metadata": {
        "id": "7pHmdCu1ZfbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers accelerate peft trl datasets sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "id": "UGjlEV55ZfdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load base LLaMA 7B\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,  # VRAM-efficient\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# LoRA config (small adapter for structure learning)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Load template dataset (structured output only)\n",
        "dataset = load_dataset(\"json\", data_files=\"template_dataset.json\")[\"train\"]\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(example):\n",
        "    input_text = f\"Query: {example['query']}\\nContext: {example['context']}\\nAnswer:\"\n",
        "    target_text = example['output']\n",
        "    input_ids = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=256).input_ids\n",
        "    labels = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=128).input_ids\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": [1]*len(input_ids), \"labels\": labels}\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sft_llama7b\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,  # only template learning\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_steps=100,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./sft_llama7b\")\n"
      ],
      "metadata": {
        "id": "cw9OVJukZfiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_fn(output, context_articles):\n",
        "    \"\"\"\n",
        "    Rule-based reward:\n",
        "    - 1 point for classification\n",
        "    - up to 2 points for citing top-3 articles\n",
        "    - 1 point for structured format\n",
        "    \"\"\"\n",
        "    reward = 0.0\n",
        "    max_reward = 3.0\n",
        "\n",
        "    # Classification\n",
        "    if \"pseudoscience\" in output.lower() or \"scientific fact\" in output.lower():\n",
        "        reward += 1.0\n",
        "\n",
        "    # Citing top-3 context\n",
        "    cited_count = sum(1 for article in context_articles if article.lower() in output.lower())\n",
        "    reward += min(cited_count, 2)\n",
        "\n",
        "    # Structured output\n",
        "    if (\"classification\" in output.lower() or \"the claim is\" in output.lower()) and \\\n",
        "       (\"based on\" in output.lower() or \"according to\" in output.lower()):\n",
        "        reward += 1.0\n",
        "\n",
        "    return min(reward / max_reward, 1.0)\n"
      ],
      "metadata": {
        "id": "ccyA5MoPoLQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "\n",
        "# Load the SFT-finetuned model for GRPO\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"./sft_llama7b\")\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# PPO configuration\n",
        "ppo_config = PPOConfig(\n",
        "    batch_size=1,\n",
        "    forward_batch_size=1,\n",
        "    ppo_epochs=4,\n",
        "    log_with=None\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=None  # manual stepping\n",
        ")\n",
        "\n",
        "# Dataset wrapper\n",
        "class PPODataset(Dataset):\n",
        "    def __init__(self, queries, top3_contexts):\n",
        "        \"\"\"\n",
        "        queries: list of user queries\n",
        "        top3_contexts: list of top-3 docs per query (already selected by reward model)\n",
        "        \"\"\"\n",
        "        self.queries = queries\n",
        "        self.contexts = top3_contexts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"query\": self.queries[idx], \"context\": self.contexts[idx]}\n",
        "\n",
        "\n",
        "queries = []\n",
        "top3_contexts = []\n",
        "ppo_dataset = PPODataset(queries, top3_contexts)\n",
        "\n",
        "\n",
        "group_size = 5\n",
        "for i in range(0, len(ppo_dataset), group_size):\n",
        "    group = [ppo_dataset[j] for j in range(i, min(i+group_size, len(ppo_dataset)))]\n",
        "\n",
        "    input_ids_list = []\n",
        "    context_list = []\n",
        "\n",
        "    # Prepare inputs\n",
        "    for item in group:\n",
        "        query = item[\"query\"]\n",
        "        context = item[\"context\"]\n",
        "        context_list.append(context)\n",
        "        input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
        "        input_ids_list.append(input_ids)\n",
        "\n",
        "    # Generate outputs\n",
        "    outputs_list = [model.generate(ids, max_new_tokens=128) for ids in input_ids_list]\n",
        "    decoded_list = [tokenizer.decode(out[0], skip_special_tokens=True) for out in outputs_list]\n",
        "\n",
        "    # Compute group-level reward\n",
        "    rewards = [reward_fn(decoded, ctx) for decoded, ctx in zip(decoded_list, context_list)]\n",
        "    group_reward = sum(rewards) / len(rewards)\n",
        "\n",
        "    # PPO step for each item in the group\n",
        "    for input_ids, outputs in zip(input_ids_list, outputs_list):\n",
        "        ppo_trainer.step(input_ids, outputs, group_reward)\n",
        "\n",
        "model.save_pretrained(\"./grpo_llama7b\")\n"
      ],
      "metadata": {
        "id": "ceBF6nGSoLS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvlRaBp_qXcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install sentence-transformers torch transformers datasets\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import CrossEncoder\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "class PairwiseRewardDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each item: query + positive doc + negative doc\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return item[\"query\"], item[\"pos_doc\"], item[\"neg_doc\"]\n",
        "\n",
        "\n",
        "train_data = [\n",
        "    {\n",
        "        \"query\": \"Does vitamin C cure the common cold?\",\n",
        "        \"pos_doc\": \"Study shows vitamin C reduces duration of cold according to clinical trials.\",\n",
        "        \"neg_doc\": \"Vitamin C is found in many fruits.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Does sugar cause diabetes?\",\n",
        "        \"pos_doc\": \"High sugar intake increases risk of type 2 diabetes as shown in research.\",\n",
        "        \"neg_doc\": \"Sugar provides energy to the body.\"\n",
        "    },\n",
        "\n",
        "]\n",
        "\n",
        "dataset = PairwiseRewardDataset(train_data)\n",
        "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "\n",
        "model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reward_model = CrossEncoder(model_name, num_labels=1)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "reward_model.to(device)\n",
        "\n",
        "margin = 1.0\n",
        "loss_fn = nn.MarginRankingLoss(margin=margin)\n",
        "optimizer = torch.optim.Adam(reward_model.parameters(), lr=2e-5)\n",
        "\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        queries, pos_docs, neg_docs = batch\n",
        "\n",
        "\n",
        "        inputs_pos = [(q, d) for q, d in zip(queries, pos_docs)]\n",
        "        inputs_neg = [(q, d) for q, d in zip(queries, neg_docs)]\n",
        "\n",
        "\n",
        "        scores_pos = reward_model.predict(inputs_pos)\n",
        "        scores_neg = reward_model.predict(inputs_neg)\n",
        "\n",
        "        scores_pos = torch.tensor(scores_pos, dtype=torch.float32, device=device)\n",
        "        scores_neg = torch.tensor(scores_neg, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Target: pos > neg â†’ y = 1\n",
        "        y = torch.ones_like(scores_pos, device=device)\n",
        "\n",
        "        loss = loss_fn(scores_pos, scores_neg, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "reward_model.save(\"trained_reward_model\")\n",
        "\n",
        "\n",
        "def select_top3_docs(query: str, docs: list, reward_model):\n",
        "    \"\"\"\n",
        "    docs: List of dicts with 'text', 'title', 'external_id'\n",
        "    Returns top-3 docs based on reward model scores\n",
        "    \"\"\"\n",
        "    if len(docs) <= 3:\n",
        "        return docs  # nothing to select\n",
        "\n",
        "    pairs = [(query, doc['text']) for doc in docs]\n",
        "    scores = reward_model.predict(pairs)\n",
        "    top3_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
        "    top3_docs = [docs[i] for i in top3_idx]\n",
        "    return top3_docs\n",
        "\n",
        "\n",
        "retrieved_docs = [\n",
        "]\n",
        "\n",
        "query = \"\"\n",
        "top3_docs = select_top3_docs(query, retrieved_docs, reward_model)\n",
        "print(\"Top 3 documents selected by reward model:\")\n",
        "for doc in top3_docs:\n",
        "    print(doc['title'], \"-\", doc['text'][:50])\n"
      ],
      "metadata": {
        "id": "doPLcq40qXeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hdgnZo7qXgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwXqPldlqXit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDngrlSGqXkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9k5DXbyxqXnA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}